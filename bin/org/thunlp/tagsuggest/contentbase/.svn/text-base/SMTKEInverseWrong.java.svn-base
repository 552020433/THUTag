package org.thunlp.tagsuggest.contentbase;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Properties;
import java.util.Set;
import java.util.Map.Entry;
import java.util.logging.Logger;

import org.thunlp.io.JsonUtil;
import org.thunlp.io.RecordReader;
import org.thunlp.misc.Counter;
import org.thunlp.misc.WeightString;
import org.thunlp.tagsuggest.common.DoubanPost;
import org.thunlp.tagsuggest.common.KeywordPost;
import org.thunlp.tagsuggest.common.Post;
import org.thunlp.tagsuggest.common.TagSuggest;
import org.thunlp.tagsuggest.common.WordFeatureExtractor;
import org.thunlp.text.Lexicon;

public class SMTKEInverseWrong implements TagSuggest {
	private static Logger LOG = Logger.getAnonymousLogger();

	private WordFeatureExtractor extractor = null;
	private Lexicon wordLex = null;

	private Properties config = new Properties();
	private static List<WeightString> EMPTY_SUGGESTION = new LinkedList<WeightString>();

	private HashMap<Integer, String> bookMap = new HashMap<Integer, String>();
	private HashMap<String, Integer> idMap = new HashMap<String, Integer>();
	private HashMap<Integer, String> bookTagMap = new HashMap<Integer, String>();
	private HashMap<String, Integer> tagIdMap = new HashMap<String, Integer>();

	private HashMap<String, Integer> df = new HashMap<String, Integer>();

	private HashMap<Integer, HashMap<Integer, Double>> proTable = new HashMap<Integer, HashMap<Integer, Double>>();
	private HashMap<Integer, HashMap<Integer, Double>> inverseTable = new HashMap<Integer, HashMap<Integer,Double>>();
	
	//private static List<WeightString> generation = new ArrayList<WeightString>();
	//private static int rightCounter = 0;
	
	@Override
	public void feedback(Post p) {
		// TODO Auto-generated method stub
	}

	@Override
	public void loadModel(String modelPath) throws IOException {
		// TODO Auto-generated method stub

		// Read book.vcb
		String bookFile = modelPath + File.separator + "book.vcb";
		BufferedReader book = new BufferedReader(new InputStreamReader(
				new FileInputStream(bookFile), "UTF-8"));
		String bookLine;
		while ((bookLine = book.readLine()) != null) {
			String[] datas = bookLine.split(" ");
			bookMap.put(Integer.parseInt(datas[0]), datas[1]);
			idMap.put(datas[1], Integer.parseInt(datas[0]));
		}
		book.close();

		// Read bookTag.vcb
		String tagFile = modelPath + File.separator + "bookTag.vcb";
		BufferedReader bookTag = new BufferedReader(new InputStreamReader(
				new FileInputStream(tagFile), "UTF-8"));
		String tagLine;
		while ((tagLine = bookTag.readLine()) != null) {
			String[] datas = tagLine.split(" ");
			bookTagMap.put(Integer.parseInt(datas[0]), datas[1]);
			tagIdMap.put(datas[1], Integer.parseInt(datas[0]));
		}
		bookTag.close();

		// Read *.t1.5
		File dir = new File(modelPath);
		Filter filter = new Filter("t1."+config.getProperty("iter", "5"));
		String files[] = dir.list(filter);
		if (files.length == 0) {
			System.out.println("*.t1.5 not exist");
			LOG.info("*.t1.5 not exist");
		}
		else{
			String word2Tag;
			String tag2Word;
			if(files[0].compareTo(files[1]) < 0){
				word2Tag = files[0];
				tag2Word = files[1];
			}
			else{
				word2Tag = files[1];
				tag2Word = files[0];
			}
			
			BufferedReader pro = new BufferedReader(new InputStreamReader(
					new FileInputStream(modelPath + File.separator +  tag2Word),
					"UTF-8"));
			String proLine;
			while ((proLine = pro.readLine()) != null) {
				String[] data = proLine.split(" ");
				if (data.length != 3)
					continue;
				int first = Integer.parseInt(data[0]);
				int second = Integer.parseInt(data[1]);
				double probability = Double.parseDouble(data[2]);
				if (first == 0 || second == 0) {
					continue;
				}
				if (proTable.containsKey(first)) {
					proTable.get(first).put(second, probability);
				} else {
					HashMap<Integer, Double> tmp = new HashMap<Integer, Double>();
					tmp.put(second, probability);
					proTable.put(first, tmp);
				}
			}
			pro.close();
			/*
			pro = new BufferedReader(new InputStreamReader(
					new FileInputStream(modelPath + File.separator + tag2Word),
					"UTF-8"));
			while ((proLine = pro.readLine()) != null) {
				String[] data = proLine.split(" ");
				if (data.length != 3)
					continue;
				int first = Integer.parseInt(data[0]);
				int second = Integer.parseInt(data[1]);
				double probability = Double.parseDouble(data[2]);
				if (first == 0 || second == 0) {
					continue;
				}
				if (inverseTable.containsKey(first)) {
					inverseTable.get(first).put(second, probability);
				} else {
					HashMap<Integer, Double> tmp = new HashMap<Integer, Double>();
					tmp.put(second, probability);
					inverseTable.put(first, tmp);
				}
				
			}
			pro.close();
			*/
		}
		
		
		// Read ti.fianl
		Filter filter2 = new Filter("cxx.ti.final");
		String files2[] = dir.list(filter2);
		if(files2.length == 0){
			System.out.println("*.ti.final not exist");
			LOG.info("*.ti.final not exist");
		}
		else{
			String word2Tag;
			String tag2Word;
			if(files2[0].compareTo(files2[1]) < 0){
				word2Tag = files2[0];
				tag2Word = files2[1];
			}
			else{
				word2Tag = files2[1];
				tag2Word = files2[0];
			}
			
			BufferedReader inverse = new BufferedReader(
					new InputStreamReader(new FileInputStream(modelPath + File.separator + word2Tag),"UTF-8"));
			String line;
			while((line = inverse.readLine()) != null){
				String[] data = line.split(" ");
				if(data.length != 3) continue;
				int first = Integer.parseInt(data[0]);
				int second = Integer.parseInt(data[1]);
				double probability = Double.parseDouble(data[2]);
				if(first == 0 || second == 0){
					continue;
				}
				if(inverseTable.containsKey(first)){
					inverseTable.get(first).put(second, probability);
				}
				else{
					HashMap<Integer, Double> tmp = new HashMap<Integer, Double>();
					tmp.put(second, probability);
					inverseTable.put(first, tmp);
				}
			}
			inverse.close();
		}
		
		
		// read wordlex
		wordLex = new Lexicon();
		String input = modelPath+"/wordlex";
		File cachedWordLexFile = new File(input);
		if (cachedWordLexFile.exists()) {
			LOG.info("Use cached lexicons");
			wordLex.loadFromFile(cachedWordLexFile);
		}
		
		
	}

	@Override
	public void setConfig(Properties config) {
		// TODO Auto-generated method stub
		this.config = config;
		extractor = new WordFeatureExtractor(config);
	}

	@Override
	public List<WeightString> suggest(Post p, StringBuilder explain) {
		// TODO Auto-generated method stub

		HashMap<Integer, Double> wordTfidf = new HashMap<Integer, Double>();
		HashMap<Integer, HashMap<Integer, Double>> LDA = new HashMap<Integer, HashMap<Integer,Double>>();
		
		String[] words = extractor.extractKeyword((KeywordPost)p, true, false, false);
		Counter<String> termFreq = new Counter<String>();
		
		HashSet<String> wordSet = new HashSet<String>();
		// calculate the word tfidf
		for (String word : words) {
			if (tagIdMap.containsKey(word)){
				wordSet.add(word);
				termFreq.inc(word, 1);
			}
		}
		Iterator<Entry<String, Long>> iter = termFreq.iterator();
		HashMap<Integer, Double> proMap = new HashMap<Integer, Double>();
		double para = Double.parseDouble(config.getProperty("harmonic_para", "1.0"));
		while (iter.hasNext()) {
			Entry<String, Long> e = iter.next();
			String word = e.getKey();
			double tf = (double) e.getValue() / (double) words.length;
			// double idf = (double)D/(double)df.get(word);
			double idf = 0.0;
			if(wordLex.getWord(word) != null){
				idf = Math.log((double) wordLex.getNumDocs()
					/ (double) wordLex.getWord(word).getDocumentFrequency());
			}
			else{
				continue;
			}
			double tfidf = tf * idf;
			int id = tagIdMap.get(word);
			if (proTable.containsKey(id)) {
				wordTfidf.put(id, tfidf);
				
				HashMap<Integer, Double> tmpMap = new HashMap<Integer, Double>();
				tmpMap.putAll(proTable.get(id));
				LDA.put(id, tmpMap);
				
				// to suggest the tags
				
				for (Entry<Integer, Double> ee : proTable.get(id).entrySet()) {
					int tagId = ee.getKey();
					
					if(inverseTable.containsKey(id) && inverseTable.get(id).containsKey(tagId)){
						//double pro = ee.getValue(); // * inverseTable.get(id).get(tagId);
						double pro = 1.0 / ( para / ee.getValue() + (1.0 - para) / inverseTable.get(id).get(tagId));
						if (proMap.containsKey(tagId)) {
							double tmp = proMap.get(tagId);
							tmp += tfidf * pro;
							proMap.remove(tagId);
							proMap.put(tagId, tmp);
						} else {
							proMap.put(tagId, tfidf * pro);
						}
					}
				/*
				// remove the words with low tfidf
				Object [] ans = proTable.get(id).entrySet().toArray();
				Comparator<Object> c = new Comparator<Object>(){
					public int compare(Object o1, Object o2) {
					    double d1 = ((Entry<Integer, Double>)o1).getValue();
					    double d2 = ((Entry<Integer, Double>)o2).getValue();
					    if(d1 < d2)return 1;
					    if(d1 == d2)return 0;
					    else return -1;
					    }
					};
				Arrays.sort(ans,c);
				
				int limit = 5;
				int top = 0;
				for (Object ee : ans) {
					top ++;
					if(top >= limit || (((Entry<Integer, Double>)ee).getValue() < 0.10)){
						break;
					}
					int tagId = ((Entry<Integer, Double>)ee).getKey();
					
					if (proMap.containsKey(tagId)) {
						double tmp = proMap.get(tagId);
						tmp += tfidf * ((Entry<Integer, Double>)ee).getValue();
						proMap.remove(tagId);
						proMap.put(tagId, tmp);
					} else {
						proMap.put(tagId, tfidf * ((Entry<Integer, Double>)ee).getValue());
					}
					*/
					/*
					// use inverseTable probability
					if(inverseTable.containsKey(tagId) && inverseTable.get(tagId).containsKey(id)){
						double pro = inverseTable.get(tagId).get(id);
						if(LDA.containsKey(id)){
							LDA.get(id).put(tagId, pro);
						}
						else{
							HashMap<Integer, Double> tmp = new HashMap<Integer, Double>();
							tmp.put(tagId, pro);
							LDA.put(id, tmp);
						}
						
						if (proMap.containsKey(tagId)) {
							double tmp = proMap.get(tagId);
							tmp += tfidf * pro;
							proMap.remove(tagId);
							proMap.put(tagId, tmp);
						} else {
							proMap.put(tagId, tfidf * pro);
						}
						
					}
					*/
				}
			}
		}
		/*
		// iteration
		double dis = 0.0;
		int count = 0;
		HashMap<Integer, Double> tagPro = new HashMap<Integer, Double>();
		do{
			dis = 0.0;
			tagPro.clear();
			count ++;
			for(Entry<Integer, HashMap<Integer, Double>> e : LDA.entrySet()){
				int id = e.getKey();
				double tfidf = wordTfidf.get(id);
				double sum = 0.0;
				for(Entry<Integer, Double> ee : e.getValue().entrySet()){
					int tagId = ee.getKey();
					if(proMap.containsKey(tagId)){
						double newPro = ee.getValue() * proMap.get(tagId);
						ee.setValue(newPro);
						sum += newPro;
					}
				}
				for(Entry<Integer, Double> ee : e.getValue().entrySet()){
					double normalized = ee.getValue() / sum;
					ee.setValue(normalized);
					int tagId = ee.getKey();
					if(tagPro.containsKey(tagId)){
						double tmp = tagPro.get(tagId);
						tmp += tfidf * normalized;
						tagPro.remove(tagId);
						tagPro.put(tagId, tmp);
					}
					else {
						tagPro.put(tagId, tfidf * normalized);
					}
				}
			}
			for(Entry<Integer, Double> e:proMap.entrySet()){
				if(tagPro.containsKey(e.getKey())){
					dis += Math.pow(tagPro.get(e.getKey()) - e.getValue(), 2.0);
				}
			}
			dis = Math.sqrt(dis);
			proMap.clear();
			proMap.putAll(tagPro);
		}while(dis > 0.0001 && count < 100);
		*/
		
		// ranking
		List<WeightString> tags = new ArrayList<WeightString>();
		for (Entry<Integer, Double> e : proMap.entrySet()) {
			String tag = bookMap.get(e.getKey());
			if(!wordSet.contains(tag)){
				tags.add(new WeightString(bookMap.get(e.getKey()), e
							.getValue()));
			}
		}
		Collections.sort(tags, new Comparator<WeightString>() {

			@Override
			public int compare(WeightString o1, WeightString o2) {
				return Double.compare(o2.weight, o1.weight);
			}

		});
		
		/*
		int rightNotInTitle = 0;
		for(int i = 0; i < tags.size() && i < 10; i ++){
			String keyword = tags.get(i).text;
			if(p.getTags().contains(keyword) && !wordSet.contains(keyword)){
				rightNotInTitle ++;
			}
		}
		generation.add(new WeightString(p.getId(), rightNotInTitle));
		*/
		
		return tags;
	}

	/*
	public static void outputGeneration(String filename) throws IOException{
		BufferedWriter outG = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(filename),"UTF-8"));
		Collections.sort(generation, new Comparator<WeightString>() {
			@Override
			public int compare(WeightString o1, WeightString o2) {
				return Double.compare(o2.weight, o1.weight);
			}
		});
		for(int i = 0; i < generation.size(); i ++){
			outG.write(generation.get(i).text + " " + generation.get(i).weight);
			outG.newLine();
			outG.flush();
		}
		outG.close();
	}
	*/
	
	
	public static void main(String[] args) throws IOException {
		SMTTagSuggest smt = new SMTTagSuggest();
		smt.loadModel("/home/cxx/smt/sample");
		RecordReader reader = new RecordReader("/home/cxx/smt/sample/test.dat");
		BufferedWriter outTag = new BufferedWriter(new OutputStreamWriter(
				new FileOutputStream("/home/cxx/smt/sample/suggest"), "UTF-8"));
		JsonUtil J = new JsonUtil();
		List<WeightString> tags;
		while (reader.next()) {
			DoubanPost p = J.fromJson(reader.value(), DoubanPost.class);
			tags = smt.suggest(p, null);
			int counter = 0;
			for (WeightString s : tags) {
				outTag.write(s.toString() + " ");
				counter++;
				if (counter == 10)
					break;
			}
			outTag.newLine();
			outTag.flush();
		}
		reader.close();
		outTag.close();
	}

}
